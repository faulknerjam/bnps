---
title: "R Code for Bayesian Trend Filtering"
output:
  html_document:
    theme: united
    toc: yes
    mathjax: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

The following document provides example code for implementing the Bayesian trend filter as described in Faulkner and Minin (2015).  You can simply copy and paste the code to your R terminal window or to a script file to run it.  We use Hamiltonion Monte Carlo (HMC) to target the posterior distributions of the parameters.  For this, we take advantage of the computing package [Stan](http://mc-stan.org/), which offers a straight-forward and efficient interface for implementing HMC.  Some preliminary setup is necessary to install the needed software, which we describe in what immediately follows. 


## Preliminary Requirements
### Install Software
All code presented here is for the R computing environment.  If you do not already have R, then please visit the [R CRAN website](http://cran.us.r-project.org/) and follow instructions for installation.  The model fitting depends on the [RStan](http://mc-stan.org/interfaces/rstan.html) interface to the [Stan](http://mc-stan.org/) computing language.  If you do not already have RStan, then please follow the [RStan Getting Started Guide](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started#prerequisites) before proceding. 

Once you have RStan installed in R, you just have to load the package using the library command.
```{r eval=FALSE}
library(rstan)
```

### Load Required Files
There is not an R package currently available for implentating our version of the Bayesian trend filter.  However, one can still run our models by loading the files with the necessary background functions.  The necessary files are found HERE :: NEED A LINK!!!  Once you have the files saved in your working directory, you need to source them to make the functions available for use.

```{r eval=FALSE}
source('btf_mods.r')
source('btf_init_funcs.r')
source('btf_base_funcs.r')
```

[//]:(Need to set up links to the files)

Now you are ready to start fitting models.

## Example Function Fits
Here we use two examples of fitting models where data are simulated from known functions.  The two functions were used in simulations presented in Faulkner and Minin (2015).  The first function is a piecewise constant function and the second is a function with varying smoothness.  These functions are best fit with models that allow adaptation to the sharp local features that the functions exhibit.  The following plot shows the true function trajectories and some generated data.  The data are drawn from normal distributions with mean equal to the true function and standard deviation of 4.5.   

PLOT HERE!!!

In Faulkner and Minin (2015) we compare three different configurations for the prior distributions on the order-*k* differences in latent function values (normal, Laplace, and horseshoe).  The normal prior is the default used in Gaussian Markov random field (GRMF) models, and is not considered locally adaptive.  The Laplace and horseshoe priors are types of shrinkage priors.  These two formulations allow some locally-adaptive behavior, with the horseshoe being the most adaptive of the two.  More extreme shrinkage priors than the horseshoe have been developed that could provide more locally-adaptive smoothing, but we currently only have code available for the Laplace and horseshoe shrinkage priors.  We encourage you to explore different prior formulations with your own modifications to our code. 


### Piecewise Constant Function
The following code sets up the piecewise constant function and generates a set of independent normally distributed observations.  For this example, the standard deviations of the observations is 4.5, which corresponds to a signal-to-noise ratio of 2.
```{r eval=FALSE}
# Set up function
nx <- 100
xv <- (1:nx)/nx
mpc <- rep(25, nx) 
mpc[xv >= 0.2 & xv < 0.4] <- 10 
mpc[xv >= 0.4 & xv < 0.6] <- 35 
mpc[xv >= 0.6] <- 15

# Generate data
set.seed(3)
pc.sd <- 4.5
pc.norm <- rnorm(n=nx, mean=mpc, sd=pc.sd)
pcdat.norm <- list(J = nx, y = pc.norm, muy = mean(pc.norm), sdy=sd(pc.norm))
```
The rstan package is built on C++ code.  Before running a model, the model code needs to be compiled.  
Next we create initial model fit objects that can be called upon many times without re-compiling.  This is not really necessary in the examples we present here, but it is very useful when calling models multiple times, such as within simulations.

```{r eval=FALSE}
# Compile code for fits
# -- GMRF
ifit.N <- bsmfit(likelihood="normal", prior="normal", order=1, data=pcdat.norm, chains=0)
# -- Laplace
ifit.L <- bsmfit(likelihood="normal", prior="laplace", order=1, data=pcdat.norm, chains=0)
# -- Horseshoe
ifit.H <- bsmfit(likelihood="normal", prior="horseshoe", order=1, data=pcdat.norm, chains=0)
		
```

Some of the model parameters may not be of interest in posterior summaries, including those the model code generates due to parameter transformations.  We can tell the `stan` function to give us output only for the parameters of interest.  We set up the following character vectors to specify the parameters of interest for later summaries.  We are really only interested in the theta parameters, but have included others for posterior diagnostic checks.


```{r eval=FALSE}
# Parameters to keep 
pars.GM1 <- c("theta", "gam","omega1","muth1", "sigma") 
pars.DE1 <- c("theta", "tau", "gam", "omega1","muth1", "sigma") 
pars.HS1 <- c("theta", "tau", "gam", "omega1","muth1", "sigma") 


```



### Varying Smooth Function
We repeat the above steps for the function with varying smoothness.  For this example, we fit second-order models using the three prior formulations.  The following code provides all of the data generation and model fitting steps in one chunk.

```{r eval=FALSE}
# Set up function
nx <- 100
xv <- (1:nx)/nx
ygfun <- function(x){
   sin(x) + 2*exp(-30*x^2)
}
gseq <- seq(-2,2,length=101)
mvs1 <- 20 + 10*ygfun(gseq)
mvs <- mvs1[-1]

# Generate data
set.seed(3)
vs.sd <- 4.5
vs.norm <- rnorm(n=nx, mean=mvs, sd=vs.sd)
vsdat.norm <- list(J = nx, y = vs.norm, muy = mean(vs.norm), sdy=sd(vs.norm))

# Compile code for fits
# -- GMRF 
ifit.norm.GM2 <- stan(model_code = norm_2_GM_code, data=vsdat.norm, chains=0)
# -- Laplace 
ifit.norm.DE2 <- stan(model_code = norm_2_DE_code, data=vsdat.norm, chains=0)
# -- Horseshoe
ifit.norm.HS2 <- stan(model_code = norm_2_HS_code, data=vsdat.norm, chains=0)


# Parameters to keep 
pars.GM2 <- c("theta", "gam","omega1","omega2","muth1", "sigma") 
pars.DE2 <- c("theta", "tau", "gam", "omega1","omega2","muth1", "sigma") 
pars.HS2 <- c("theta", "tau", "gam", "omega1","omega2","muth1", "sigma") 



```



### Diagnostic Plots

plotTrace


### Posterior Median Plots

plotTrend

### Parallelized Version
These models have a large number of parameters, so MCMC can take a long time to run. The required time can be greatly reduced by running processes in paralell.  The following is an example of how you may parallelize your 

### Changing Order

## Coal Mining Example
Here we provide the code to reproduce the results from the example on coal mine disasters given in Faulkner and Minin (2015).  


### Data

The data are number of disasters per year for the years 1851-1962.  The following code sets up a vector of the annual event counts and then creates a list with the data components needed for the models to run.

```{r eval=FALSE}
ncount <- c(4,5,4,1,0,4,3,4,0,6,3,3,4,0,2,6,3,3,5,4,5,3,1,4,4,1,5,5,3,4,2,5,2,2,3,4,2,1,3,2,2,1,1,1,1,3,0,0,1,0,1,1,0,0,3,1,0,3,2,2,0,1,1,1,0,1,0,1,0,0,0,2,1,0,0,0,1,1,0,2,3,3,1,1,2,1,1,1,1,2,3,3,0,0,0,1,4,0,0,0,1,0,0,0,0,0,1,0,0,1,0,1)

coal_dat <- list(J = length(ncount), y = ncount)

```

### Run Models

In the paper we compare three different configurations for the prior distributions on the order-*k* differences in latent function values (normal, Laplace, and horseshoe).





### Posterior Plots



## References

Faulkner, J.R., and V.N. Minin.  2015.  Bayesian trend filtering: adaptive temporal smoothing with shrinkage priors. arXiv preprint.




